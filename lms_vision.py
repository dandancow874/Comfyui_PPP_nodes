import torch
import numpy as np
from PIL import Image
import base64
import io
import requests
import subprocess
import json
import os
import logging
import time

logger = logging.getLogger("LMS_Controller")

class LMS_CLI_Handler:
    """
    è´Ÿè´£ä¸ LM Studio å‘½ä»¤è¡Œäº¤äº’ (è·¨å¹³å°å…¼å®¹ç‰ˆ)
    """
    _model_cache = None
    _last_cache_time = 0
    CACHE_TTL = 10 

    @staticmethod
    def get_lms_path():
        # --- Windows é€»è¾‘ ---
        if os.name == 'nt':
            user_home = os.path.expanduser("~")
            candidates = [
                os.path.join(user_home, ".lmstudio", "bin", "lms.exe"),
                os.path.join(user_home, "AppData", "Local", "LM-Studio", "app", "bin", "lms.exe")
            ]
            for path in candidates:
                if os.path.exists(path):
                    return path
            return "lms" # å¦‚æœæ‰¾ä¸åˆ°è·¯å¾„ï¼Œå°è¯•ç›´æ¥è°ƒç”¨å‘½ä»¤
        
        # --- Mac/Linux é€»è¾‘ ---
        else:
            # åœ¨ Mac ä¸Šï¼Œåªè¦ç”¨æˆ·ç‚¹äº† "Install lms to PATH"ï¼Œç›´æ¥ç”¨ lms å³å¯
            # ä¹Ÿå¯ä»¥æ£€æŸ¥ä¸€ä¸‹é»˜è®¤è·¯å¾„ä½œä¸ºå…œåº•
            return "lms"

    @staticmethod
    def run_cmd(args, timeout=60):
        lms_path = LMS_CLI_Handler.get_lms_path()
        cmd = [lms_path] + args
        
        startupinfo = None
        if os.name == 'nt':
            startupinfo = subprocess.STARTUPINFO()
            startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW
            startupinfo.wShowWindow = subprocess.SW_HIDE

        try:
            # [å…³é”®ä¿®æ”¹] ä½¿ç”¨ Popen æ›¿ä»£ runï¼Œä»¥ä¾¿éé˜»å¡æ‰§è¡Œ
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                encoding='utf-8',
                errors='replace',
                startupinfo=startupinfo
            )
            
            start_time = time.time()
            while True:
                # 1. æ£€æŸ¥å­è¿›ç¨‹æ˜¯å¦ç»“æŸ
                retcode = process.poll()
                if retcode is not None:
                    stdout, stderr = process.communicate()
                    return retcode == 0, stdout, stderr
                
                # 2. æ£€æŸ¥æ˜¯å¦è¶…æ—¶
                if time.time() - start_time > timeout:
                    process.kill()
                    return False, "", f"Timeout after {timeout}s"
                
                # 3. [æ ¸å¿ƒ] æ£€æŸ¥ ComfyUI æ˜¯å¦å‘å‡ºäº†å–æ¶ˆä¿¡å· (æŠ›å‡ºå¼‚å¸¸)
                # ComfyUI çš„ä¸­æ–­æœºåˆ¶é€šå¸¸æ˜¯é€šè¿‡åœ¨ Python çº¿ç¨‹ä¸­æŠ›å‡º SystemExit æˆ– KeyboardInterrupt
                # æˆ‘ä»¬è¿™é‡Œè™½ç„¶åœ¨ while å¾ªç¯ï¼Œä½† Python è§£é‡Šå™¨æœ‰æœºä¼šå“åº”ä¿¡å·
                # å¦‚æœç”¨æˆ·ç‹‚æŒ‰åœæ­¢ï¼ŒComfyUI å¯èƒ½ä¼šå°è¯• kill è¿™ä¸ªèŠ‚ç‚¹çš„ worker çº¿ç¨‹
                
                time.sleep(0.5) # è®©å‡º CPUï¼Œç»™ä¸­æ–­ä¿¡å·æœºä¼š
                
        except KeyboardInterrupt:
            # æ•è·ç”¨æˆ·çš„åœæ­¢æ“ä½œ
            if process:
                process.kill()
            logger.warning("User cancelled the operation.")
            raise # é‡æ–°æŠ›å‡ºï¼Œè®© ComfyUI çŸ¥é“ä»»åŠ¡å·²å–æ¶ˆ
            
        except Exception as e:
            if process:
                process.kill()
            return False, "", str(e)

    @classmethod
    def get_models(cls):
        # å¦‚æœç¼“å­˜æœ‰æ•ˆï¼Œç›´æ¥è¿”å›
        if cls._model_cache and (time.time() - cls._last_cache_time < cls.CACHE_TTL):
            return cls._model_cache

        success, stdout, stderr = cls.run_cmd(["ls"], timeout=5)
        if not success:
            logger.error(f"LMS LS Error: {stderr}")
            return ["Error: lms ls failed"]

        models = []
        lines = stdout.strip().splitlines()
        
        # å…³é”®è¯é»‘åå• (è¿‡æ»¤æ‰è¡¨å¤´å’Œæ— å…³ä¿¡æ¯)
        BLACKLIST = {
            "size", "ram", "type", "architecture", "model", "path", 
            "llm", "llms", "embedding", "embeddings", "vision", "image",
            "name", "loading", "fetching", "downloaded", "bytes", "date",
            "publisher", "repository", "you", "have", "features", "primary", "gpu"
        }
        
        for line in lines:
            line = line.strip()
            if not line: continue
            # è¿‡æ»¤æ‰åˆ†éš”çº¿
            if all(c in "-=*" for c in line): continue
            
            parts = line.split()
            if not parts: continue
            
            # ç¬¬ä¸€åˆ—é€šå¸¸æ˜¯æ¨¡å‹å
            raw_name = parts[0]
            raw_lower = raw_name.lower()
            
            # è¿‡æ»¤è¡¨å¤´
            if raw_lower.rstrip(":") in BLACKLIST: continue
            if raw_lower[0].isdigit() and ("gb" in raw_lower or "mb" in raw_lower): continue
            
            # æå–å¹²å‡€çš„æ¨¡å‹å
            clean_name = raw_name
            # å¦‚æœæ˜¯è·¯å¾„ï¼Œåªå–æœ€åçš„æ–‡ä»¶å
            if "/" in clean_name or "\\" in clean_name: 
                 clean_name = os.path.basename(clean_name)
            
            # å¦‚æœæœ‰ .gguf åç¼€ï¼Œä¿ç•™å®ƒ (ä¸ºäº†ç²¾ç¡®åŒ¹é…)ï¼Œæˆ–è€…å»æ‰å®ƒ (ä¸ºäº†ç¾è§‚)
            # å»ºè®®ï¼šå¦‚æœ LM Studio åŠ è½½å‘½ä»¤éœ€è¦å®Œæ•´åå­—ï¼Œæœ€å¥½ä¿ç•™ .gguf
            # ä½†ç”¨æˆ·ä¹ æƒ¯åªçœ‹åå­—ï¼Œè¿™é‡Œåšä¸ªæŠ˜ä¸­ï¼š
            # å¦‚æœåå­—å¤ªé•¿ï¼Œæˆ–è€…åŒ…å«å®Œæ•´è·¯å¾„ï¼ŒLM Studio çš„ `lms load` é€šå¸¸æ”¯æŒæ¨¡ç³ŠåŒ¹é…ï¼Œä½†æœ€å¥½æä¾›å®Œæ•´çš„ `publisher/repo/file` æ ¼å¼
            
            # [å…³é”®ä¿®æ”¹] ä¸ºäº†è§£å†³ "not found" é—®é¢˜ï¼Œæˆ‘ä»¬å°è¯•æŠ“å–å®Œæ•´çš„ä¸€è¡Œä½œä¸ºå€™é€‰ï¼Œæˆ–è€…æŠ“å–æ›´ç²¾ç¡®çš„æ ‡è¯†ç¬¦
            # ä½† `lms ls` çš„è¾“å‡ºæ ¼å¼å¯¹é½å¾ˆä¹±ã€‚
            # ç°åœ¨çš„ç­–ç•¥ï¼šå¦‚æœè¿™ä¸€è¡ŒåŒ…å« ">" (è¡¨ç¤ºå½“å‰é€‰ä¸­çš„)ï¼Œå»æ‰å®ƒ
            if clean_name == ">":
                if len(parts) > 1:
                    clean_name = parts[1]
                else:
                    continue
            
            if len(clean_name) < 2: continue
            models.append(clean_name)

        unique_models = sorted(list(set(models)))
        if not unique_models: unique_models = ["No models found"]
        cls._model_cache = unique_models
        cls._last_cache_time = time.time()
        return unique_models

    @classmethod
    def load_model(cls, model_name, identifier, gpu_ratio=1.0, context_length=2048):
        # ç®€å•å¤„ç†ï¼šå¦‚æœä¹‹å‰åŠ è½½çš„å°±æ˜¯è¿™ä¸ªæ¨¡å‹ï¼Œä¸”å‚æ•°æ²¡å˜ï¼Œå°±è·³è¿‡
        # æ³¨æ„ï¼šè¿™é‡Œä»…ä»…æ˜¯ç®€å•çš„ç¼“å­˜æ£€æŸ¥ï¼Œæ›´ä¸¥è°¨çš„åšæ³•æ˜¯æŸ¥è¯¢ lms ps
        # ä½†è€ƒè™‘åˆ° lms ps è§£æå¤æ‚ï¼Œè¿™é‡Œå…ˆç”¨ç±»å˜é‡ç¼“å­˜
        
        logger.info(f"LMS: Loading '{model_name}' (GPU: {gpu_ratio}, Ctx: {context_length})...")
        
        # æ„é€ å‚æ•°
        # æ³¨æ„ï¼šLM Studio ç‰ˆæœ¬ä¸åŒå‚æ•°å¯èƒ½ä¸åŒï¼Œè¿™é‡Œä½¿ç”¨è¾ƒé€šç”¨çš„å‚æ•°
        # å¦‚æœæ˜¯ 0.3.x ç‰ˆæœ¬ï¼Œ--gpu å¯èƒ½å˜æˆäº† --gpu-offload-ratio
        # ä½†ç›®å‰ --gpu ä»ç„¶å…¼å®¹å¤§å¤šæ•°ç‰ˆæœ¬
        
        gpu_arg = "max" if gpu_ratio >= 1.0 else str(gpu_ratio)
        if gpu_ratio <= 0: gpu_arg = "0"

        args = ["load", model_name, "--identifier", identifier, "--gpu", gpu_arg, "--context-length", str(context_length)]
        
        # [Debug] æ‰“å°å®Œæ•´å‘½ä»¤ï¼Œæ–¹ä¾¿æ’æŸ¥
        logger.info(f"Executing: lms {' '.join(args)}")

        # [ä¿®å¤] å¢åŠ è¶…æ—¶æ—¶é—´åˆ° 300ç§’ (5åˆ†é’Ÿ)ï¼Œé˜²æ­¢å¤§æ¨¡å‹åŠ è½½æ…¢å¯¼è‡´è¶…æ—¶
        success, stdout, stderr = cls.run_cmd(args, timeout=300)
        
        if not success:
            # [å…³é”®] å¢åŠ å¯¹ "Model not found" çš„ç‰¹å¼‚æ€§å¤„ç†
            # å¾ˆå¤šæ—¶å€™æ˜¯å› ä¸ºåå­—ä¸åŒ¹é…ï¼Œæˆ–è€…éœ€è¦å…¨è·¯å¾„
            if "not found" in stderr.lower() or "did you mean" in stderr.lower():
                logger.warning(f"Model '{model_name}' not found directly. Attempting fuzzy search...")
                # è¿™é‡Œå¯ä»¥å°è¯•è‡ªåŠ¨çº æ­£ï¼Œä½†ç›®å‰å…ˆç»™å‡ºæ˜ç¡®æç¤º
                logger.error(f"Suggest: Run 'lms ls' in terminal to check the exact name.")
            
            logger.error(f"LMS Load Error (stderr): {stderr}")
            logger.error(f"LMS Load Output (stdout): {stdout}")
            return False
        
        # å³ä½¿ returncode == 0ï¼Œæœ‰æ—¶å€™ lms ä¹Ÿä¼šè¾“å‡ºé”™è¯¯ä¿¡æ¯åˆ° stdout
        if "error" in stdout.lower() or "failed" in stdout.lower():
             logger.warning(f"LMS Load returned success but output contains error keywords:\n{stdout}")

        logger.info(f"LMS Load Success: {stdout.strip()}")
        return True

    @classmethod
    def unload_all(cls):
        # ... (unload_all ä¿æŒä¸å˜) ...
        success, _, stderr = cls.run_cmd(["unload", "--all"], timeout=20)
        return success

class LMS_VisionController:
    _current_loaded_model = None 
    _current_gpu_ratio = 1.0
    _current_context = 2048

    def __init__(self):
        self.cli = LMS_CLI_Handler()

    @classmethod
    def INPUT_TYPES(cls):
        model_list = LMS_CLI_Handler.get_models()
        return {
            "required": {
                # [ä¿®æ”¹ç‚¹1] image å·²ç»ç§»åˆ°äº† optionalï¼Œè¿™é‡Œåªä¿ç•™å…¶ä»–å¿…å¡«é¡¹
                "user_prompt": ("STRING", {"multiline": True, "default": "Describe the content of the images/video."}),
                "model_name": (model_list,),
                "max_total_images": ("INT", {"default": 8, "min": 1, "max": 64}),
                "gpu_offload": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 1.0, "step": 0.05}),
                "context_length": ("INT", {"default": 8192, "min": 512, "max": 32768}),
                "max_image_side": ("INT", {"default": 1024, "min": 256, "max": 4096}),
                "max_tokens": ("INT", {"default": 1024, "min": 1, "max": 32768}),
                "temperature": ("FLOAT", {"default": 0.6, "min": 0.0, "max": 2.0, "step": 0.05}),
                "seed": ("INT", {"default": 0, "min": 0, "max": 0xffffffffffffffff}),
                "unload_after": ("BOOLEAN", {"default": False}),
            },
            "optional": {
                # [ä¿®æ”¹ç‚¹2] image ç°åœ¨æ˜¯å¯é€‰çš„äº†ï¼
                "image": ("IMAGE",),
                "image_2": ("IMAGE",),
                "image_3": ("IMAGE",),
                "video_frames": ("IMAGE",), 
                "system_prompt": ("STRING", {"multiline": True, "default": "You are a helpful AI assistant."}),
                "base_url": ("STRING", {"default": "http://localhost:1234/v1"}),
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("response_text",)
    FUNCTION = "generate_content"
    CATEGORY = "PPP_nodes/LM Studio"

    def process_image(self, tensor_img, max_side):
        try:
            img_np = (tensor_img.cpu().numpy() * 255).clip(0, 255).astype(np.uint8)
            pil_img = Image.fromarray(img_np)
            if pil_img.mode != 'RGB': pil_img = pil_img.convert('RGB')
            width, height = pil_img.size
            if max(width, height) > max_side:
                ratio = max_side / max(width, height)
                new_size = (int(width * ratio), int(height * ratio))
                pil_img = pil_img.resize(new_size, Image.Resampling.LANCZOS)
            buffer = io.BytesIO()
            pil_img.save(buffer, format="JPEG", quality=85)
            return base64.b64encode(buffer.getvalue()).decode('utf-8')
        except Exception as e:
            logger.error(f"Image processing error: {e}")
            return None

    def generate_content(self, user_prompt, model_name, max_total_images, gpu_offload, context_length, max_image_side,
                         max_tokens, temperature, seed, unload_after, 
                         image=None, image_2=None, image_3=None, video_frames=None,
                         system_prompt="", base_url="http://localhost:1234/v1", **kwargs):
        
        if "http" not in base_url: base_url = "http://localhost:1234/v1"
        IDENTIFIER = "comfy_vlm_worker"
        
        # 1. æ”¶é›†å›¾ç‰‡
        all_tensors = []
        if image is not None:
            for i in range(image.shape[0]): all_tensors.append(image[i])
        if image_2 is not None:
            for i in range(image_2.shape[0]): all_tensors.append(image_2[i])
        if image_3 is not None:
            for i in range(image_3.shape[0]): all_tensors.append(image_3[i])
        if video_frames is not None:
            for i in range(video_frames.shape[0]): all_tensors.append(video_frames[i])
        
        total_count = len(all_tensors)
        
        # --- [ä¿®æ”¹ç‚¹]ï¼šä¸å†å› ä¸º total_count == 0 è€ŒæŠ¥é”™ï¼Œè€Œæ˜¯è®°å½•æ—¥å¿— ---
        if total_count == 0:
            logger.info("No images detected. Running in Text-Only (Chat) mode.")
        else:
            logger.info(f"Processing {total_count} images for Vision mode.")
        
        # 2. æŠ½å¸§ (ä»…å½“æœ‰å›¾ç‰‡æ—¶æ‰§è¡Œ)
        final_tensors = []
        if total_count > 0:
            if total_count > max_total_images:
                indices = np.linspace(0, total_count - 1, max_total_images, dtype=int)
                final_tensors = [all_tensors[i] for i in indices]
            else:
                final_tensors = all_tensors

        # 3. è½¬ Base64 (ä»…å½“æœ‰å›¾ç‰‡æ—¶æ‰§è¡Œ)
        image_content_list = []
        if final_tensors:
            for tensor in final_tensors:
                b64 = self.process_image(tensor, max_image_side)
                if b64:
                    image_content_list.append({
                        "type": "image_url",
                        "image_url": {"url": f"data:image/jpeg;base64,{b64}"}
                    })

        # 4. åŠ è½½æ¨¡å‹ (ä¿æŒä¸å˜)
        needs_reload = (
            LMS_VisionController._current_loaded_model != model_name or
            abs(LMS_VisionController._current_gpu_ratio - gpu_offload) > 0.01 or 
            LMS_VisionController._current_context != context_length
        )

        if needs_reload:
            logger.info(f"Model change detected or parameters changed. Unloading old model...")
            self.cli.unload_all()
            time.sleep(2.0) # å¢åŠ ç­‰å¾…æ—¶é—´ï¼Œç¡®ä¿ç«¯å£é‡Šæ”¾
            
            logger.info(f"Loading new model: {model_name}")
            success = self.cli.load_model(model_name, IDENTIFIER, gpu_ratio=gpu_offload, context_length=context_length)
            
            if success:
                LMS_VisionController._current_loaded_model = model_name
                LMS_VisionController._current_gpu_ratio = gpu_offload
                LMS_VisionController._current_context = context_length
                # å†æ¬¡ç­‰å¾…ï¼Œç¡®ä¿æ¨¡å‹å®Œå…¨å°±ç»ª
                time.sleep(3.0) 
            else:
                err_msg = f"Error: Failed to load model '{model_name}'. Check ComfyUI console for detailed lms output."
                logger.error(err_msg)
                return (err_msg,)
        
        # å³ä½¿ä¸éœ€è¦é‡æ–°åŠ è½½ï¼Œå¦‚æœæ¨¡å‹çŠ¶æ€æ˜¯ None (æ¯”å¦‚åˆšå¯åŠ¨ ComfyUI)ï¼Œä¹Ÿåº”è¯¥å°è¯•åŠ è½½
        elif LMS_VisionController._current_loaded_model is None:
             logger.info(f"Initial model load: {model_name}")
             success = self.cli.load_model(model_name, IDENTIFIER, gpu_ratio=gpu_offload, context_length=context_length)
             if success:
                LMS_VisionController._current_loaded_model = model_name
                LMS_VisionController._current_gpu_ratio = gpu_offload
                LMS_VisionController._current_context = context_length
                time.sleep(3.0)
             else:
                return (f"Error: Failed to load model '{model_name}'.",)

        # 5. æ„å»º Payload [æ ¸å¿ƒä¿®æ”¹ï¼šåŒºåˆ†çº¯æ–‡æœ¬å’Œå¤šæ¨¡æ€]
        user_content = ""
        
        if len(image_content_list) > 0:
            # è§†è§‰æ¨¡å¼ï¼šcontent æ˜¯ä¸€ä¸ªåˆ—è¡¨ [{"type":"text"}, {"type":"image_url"}...]
            user_content = [{"type": "text", "text": user_prompt}] + image_content_list
        else:
            # çº¯æ–‡æœ¬æ¨¡å¼ï¼šcontent åªæ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²
            # è¿™æ ·å…¼å®¹æ€§æœ€å¥½ï¼Œèƒ½æ”¯æŒä¸æ”¯æŒ Vision çš„çº¯æ–‡æœ¬æ¨¡å‹ (å¦‚ Llama 3, Mistral)
            user_content = user_prompt

        payload = {
            "model": IDENTIFIER,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content}
            ],
            "temperature": temperature,
            "max_tokens": max_tokens,
            "seed": seed,
            "stream": False
        }

        # 6. å‘é€è¯·æ±‚
        content = ""
        try:
            api_endpoint = f"{base_url.rstrip('/')}/chat/completions"
            
            # æ‰“å°æ—¥å¿—è®©ç”¨æˆ·çŸ¥é“ç°åœ¨æ˜¯ä»€ä¹ˆæ¨¡å¼
            mode_str = "Vision Mode" if len(image_content_list) > 0 else "Text-Only Mode"
            logger.info(f"Sending request ({mode_str})...")
            
            response = requests.post(api_endpoint, headers={"Content-Type": "application/json"}, json=payload, timeout=300)
            if response.status_code == 200:
                result = response.json()
                if 'choices' in result and len(result['choices']) > 0:
                    content = result['choices'][0]['message']['content']
                else:
                    content = "Error: Empty response."
            else:
                content = f"API Error {response.status_code}: {response.text}"
                logger.error(content)
        except Exception as e:
            content = f"Connection Error: {str(e)}"
            logger.error(content)

        if unload_after:
            self.cli.unload_all()
            LMS_VisionController._current_loaded_model = None

        return (content,)

# ==========================================
# æ–°å¢åŠŸèƒ½ï¼šPrompt ç®¡ç†ç³»ç»Ÿ
# ==========================================

# å®šä¹‰ prompt å­˜å‚¨çš„æ ¹ç›®å½• (åœ¨å½“å‰èŠ‚ç‚¹æ–‡ä»¶å¤¹ä¸‹è‡ªåŠ¨åˆ›å»º prompts æ–‡ä»¶å¤¹)
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PROMPTS_DIR = os.path.join(CURRENT_DIR, "prompts")

# å¦‚æœæ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œè‡ªåŠ¨åˆ›å»º
if not os.path.exists(PROMPTS_DIR):
    try:
        os.makedirs(PROMPTS_DIR)
        logger.info(f"Created prompts directory at: {PROMPTS_DIR}")
    except Exception as e:
        logger.error(f"Failed to create prompts directory: {e}")

class LMS_LoadPrompt:
    """
    è¯»å–èŠ‚ç‚¹ç›®å½•ä¸‹çš„ prompt æ–‡ä»¶ (.txt, .json)
    æ”¯æŒå­æ–‡ä»¶å¤¹ï¼Œæ”¯æŒä¸‹æ‹‰æœç´¢
    """
    @classmethod
    def INPUT_TYPES(cls):
        # æ¯æ¬¡åŠ è½½èŠ‚ç‚¹æ—¶ï¼Œéå†ç›®å½•è·å–æ–‡ä»¶åˆ—è¡¨
        files = []
        if os.path.exists(PROMPTS_DIR):
            for root, dirs, files_in_dir in os.walk(PROMPTS_DIR):
                for file in files_in_dir:
                    if file.lower().endswith((".txt", ".json")):
                        # è·å–ç›¸å¯¹è·¯å¾„ï¼Œä¾‹å¦‚ "é£æ ¼\èµ›åšæœ‹å…‹.txt"
                        full_path = os.path.join(root, file)
                        rel_path = os.path.relpath(full_path, PROMPTS_DIR)
                        files.append(rel_path)
        
        # æ’åºï¼Œä¿è¯åˆ—è¡¨æ•´é½
        files.sort()
        
        if not files:
            files = ["No prompts found.txt"]

        return {
            "required": {
                "prompt_file": (files,),
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("prompt_text",)
    FUNCTION = "load_file"
    CATEGORY = "PPP_nodes/Prompt"

    def load_file(self, prompt_file):
        file_path = os.path.join(PROMPTS_DIR, prompt_file)
        
        content = ""
        if not os.path.exists(file_path):
            logger.warning(f"File not found: {file_path}")
            return ("",)

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            logger.info(f"Loaded prompt from: {prompt_file}")
        except Exception as e:
            logger.error(f"Error reading file: {e}")
            content = f"Error reading file: {str(e)}"

        return (content,)

class LMS_SavePrompt:
    """
    ä¿å­˜æ–‡æœ¬åˆ°æ–‡ä»¶
    æ”¯æŒè‡ªåŠ¨åˆ›å»ºå­æ–‡ä»¶å¤¹ (ä¾‹å¦‚è¾“å…¥: è¯ç¥\åæ¨.txt)
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "text": ("STRING", {"forceInput": True}),  # æ¥æ”¶æ¥è‡ªå…¶ä»–èŠ‚ç‚¹çš„æ–‡æœ¬
                "filename": ("STRING", {"default": "folder/my_prompt.txt", "multiline": False}),
            },
            "optional": {
                "mode": (["overwrite", "append"],), # è¦†ç›–æ¨¡å¼ æˆ– è¿½åŠ æ¨¡å¼
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("saved_text",)
    OUTPUT_NODE = True
    FUNCTION = "save_file"
    CATEGORY = "PPP_nodes/Prompt"

    def save_file(self, text, filename, mode="overwrite"):
        # è§„èŒƒåŒ–è·¯å¾„ï¼Œå¤„ç† Windows çš„åæ–œæ 
        filename = filename.replace("\\", "/")
        
        # é˜²æ­¢ä¿å­˜åˆ°çˆ¶ç›®å½• (å®‰å…¨æ£€æŸ¥)
        if ".." in filename:
            logger.warning("Attempted path traversal. Saving to root instead.")
            filename = os.path.basename(filename)

        full_path = os.path.join(PROMPTS_DIR, filename)
        
        # ç¡®ä¿å­æ–‡ä»¶å¤¹å­˜åœ¨
        directory = os.path.dirname(full_path)
        if directory and not os.path.exists(directory):
            try:
                os.makedirs(directory)
                logger.info(f"Created sub-directory: {directory}")
            except Exception as e:
                logger.error(f"Failed to create directory: {e}")
                return (text,)

        # å†™å…¥æ–‡ä»¶
        try:
            write_mode = 'w' if mode == "overwrite" else 'a'
            # å¦‚æœæ˜¯è¿½åŠ æ¨¡å¼ï¼Œå…ˆåŠ ä¸ªæ¢è¡Œç¬¦
            content_to_write = text
            if mode == "append" and os.path.exists(full_path):
                content_to_write = "\n" + text

            with open(full_path, write_mode, encoding='utf-8') as f:
                f.write(content_to_write)
            
            logger.info(f"Saved prompt to: {full_path}")
        except Exception as e:
            logger.error(f"Error saving file: {e}")

        return (text,)

# ==========================================
# æ³¨å†ŒèŠ‚ç‚¹ (è¯·æ›´æ–°åŸæœ¬åº•éƒ¨çš„ MAPPINGS)
# ==========================================

# 1. æ‰¾åˆ°ä½ åŸæœ¬ä»£ç é‡Œçš„ NODE_CLASS_MAPPINGS = { ... }
# 2. å°†å…¶æ›¿æ¢æˆ–åˆå¹¶ä¸ºä»¥ä¸‹å†…å®¹ï¼š

NODE_CLASS_MAPPINGS = {
    "LMS_VisionController": LMS_VisionController,
    "LMS_LoadPrompt": LMS_LoadPrompt,
    "LMS_SavePrompt": LMS_SavePrompt
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "LMS_VisionController": "LM Studio VLM",
    "LMS_LoadPrompt": "ğŸ“‚ Load Prompt",
    "LMS_SavePrompt": "ğŸ’¾ Save Prompt"
}




