import torch
import numpy as np
from PIL import Image
import base64
import io
import requests
import subprocess
import json
import os
import logging
import time

logger = logging.getLogger("LMS_Controller")

class LMS_CLI_Handler:
    """
    è´Ÿè´£ä¸ LM Studio å‘½ä»¤è¡Œäº¤äº’ (è·¨å¹³å°å…¼å®¹ç‰ˆ)
    """
    _model_cache = None
    _last_cache_time = 0
    CACHE_TTL = 10 

    @staticmethod
    def get_lms_path():
        # --- Windows é€»è¾‘ ---
        if os.name == 'nt':
            user_home = os.path.expanduser("~")
            candidates = [
                os.path.join(user_home, ".lmstudio", "bin", "lms.exe"),
                os.path.join(user_home, "AppData", "Local", "LM-Studio", "app", "bin", "lms.exe")
            ]
            for path in candidates:
                if os.path.exists(path):
                    return path
            return "lms" # å¦‚æœæ‰¾ä¸åˆ°è·¯å¾„ï¼Œå°è¯•ç›´æ¥è°ƒç”¨å‘½ä»¤
        
        # --- Mac/Linux é€»è¾‘ ---
        else:
            # åœ¨ Mac ä¸Šï¼Œåªè¦ç”¨æˆ·ç‚¹äº† "Install lms to PATH"ï¼Œç›´æ¥ç”¨ lms å³å¯
            # ä¹Ÿå¯ä»¥æ£€æŸ¥ä¸€ä¸‹é»˜è®¤è·¯å¾„ä½œä¸ºå…œåº•
            return "lms"

    @staticmethod
    def run_cmd(args, timeout=30):
        lms_path = LMS_CLI_Handler.get_lms_path()
        cmd = [lms_path] + args
        
        startupinfo = None
        # [å…³é”®ä¿®å¤] åªæœ‰ Windows æ‰éœ€è¦é…ç½® startupinfo æ¥éšè—é»‘æ¡†
        # Mac/Linux ä¸éœ€è¦è¿™ä¸ªï¼ŒåŠ äº†åè€Œä¼šæŠ¥é”™
        if os.name == 'nt':
            startupinfo = subprocess.STARTUPINFO()
            startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW
            startupinfo.wShowWindow = subprocess.SW_HIDE

        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=timeout,
                encoding='utf-8',
                errors='replace',
                startupinfo=startupinfo # Macä¸‹è¿™æ˜¯ Noneï¼Œä¸ä¼šæŠ¥é”™
            )
            return result.returncode == 0, result.stdout, result.stderr
        except Exception as e:
            return False, "", str(e)

    @classmethod
    def get_models(cls):
        # ... (get_models çš„å†…å®¹ä¿æŒä¸å˜ï¼Œç›´æ¥å¤åˆ¶ä¹‹å‰çš„å³å¯) ...
        # ä¸ºäº†èŠ‚çœç¯‡å¹…ï¼Œè¿™é‡Œå‡è®¾ä½ ä¿ç•™äº†ä¹‹å‰ä¿®æ­£å¥½çš„ get_models ä»£ç 
        if cls._model_cache and (time.time() - cls._last_cache_time < cls.CACHE_TTL):
            return cls._model_cache

        success, stdout, stderr = cls.run_cmd(["ls"], timeout=5)
        if not success:
            logger.error(f"LMS LS Error: {stderr}")
            return ["Error: lms ls failed"]

        models = []
        lines = stdout.strip().splitlines()
        BLACKLIST = {
            "size", "ram", "type", "architecture", "model", "path", 
            "llm", "llms", "embedding", "embeddings", "vision", "image",
            "name", "loading", "fetching", "downloaded", "bytes", "date",
            "publisher", "repository", "you", "have", "features", "primary", "gpu"
        }
        for line in lines:
            line = line.strip()
            if not line: continue
            if all(c in "-=*" for c in line): continue
            parts = line.split()
            if not parts: continue
            raw_name = parts[0]
            raw_lower = raw_name.lower()
            if raw_lower.rstrip(":") in BLACKLIST: continue
            if raw_lower[0].isdigit() and ("gb" in raw_lower or "mb" in raw_lower): continue
            clean_name = raw_name
            if "/" in clean_name: clean_name = clean_name.split("/")[-1]
            if clean_name.lower().endswith(".gguf"): clean_name = clean_name[:-5]
            if len(clean_name) < 2: continue
            models.append(clean_name)
        unique_models = sorted(list(set(models)))
        if not unique_models: unique_models = ["No models found"]
        cls._model_cache = unique_models
        cls._last_cache_time = time.time()
        return unique_models

    @classmethod
    def load_model(cls, model_name, identifier, gpu_ratio=1.0, context_length=2048):
        # ... (load_model ä¿æŒä¸å˜) ...
        logger.info(f"LMS: Loading '{model_name}' (GPU: {gpu_ratio}, Ctx: {context_length})...")
        gpu_arg = "max" if gpu_ratio >= 1.0 else str(gpu_ratio)
        if gpu_ratio <= 0: gpu_arg = "0"
        args = ["load", model_name, "--identifier", identifier, "--gpu", gpu_arg, "--context-length", str(context_length)]
        success, stdout, stderr = cls.run_cmd(args, timeout=180)
        if not success: logger.error(f"LMS Load Error: {stderr}")
        return success

    @classmethod
    def unload_all(cls):
        # ... (unload_all ä¿æŒä¸å˜) ...
        success, _, stderr = cls.run_cmd(["unload", "--all"], timeout=20)
        return success

class LMS_VisionController:
    _current_loaded_model = None 
    _current_gpu_ratio = 1.0
    _current_context = 2048

    def __init__(self):
        self.cli = LMS_CLI_Handler()

    @classmethod
    def INPUT_TYPES(cls):
        model_list = LMS_CLI_Handler.get_models()
        return {
            "required": {
                # [ä¿®æ”¹ç‚¹1] image å·²ç»ç§»åˆ°äº† optionalï¼Œè¿™é‡Œåªä¿ç•™å…¶ä»–å¿…å¡«é¡¹
                "user_prompt": ("STRING", {"multiline": True, "default": "Describe the content of the images/video."}),
                "model_name": (model_list,),
                "max_total_images": ("INT", {"default": 8, "min": 1, "max": 64}),
                "gpu_offload": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 1.0, "step": 0.05}),
                "context_length": ("INT", {"default": 8192, "min": 512, "max": 32768}),
                "max_image_side": ("INT", {"default": 1024, "min": 256, "max": 4096}),
                "max_tokens": ("INT", {"default": 1024, "min": 1, "max": 32768}),
                "temperature": ("FLOAT", {"default": 0.6, "min": 0.0, "max": 2.0, "step": 0.05}),
                "seed": ("INT", {"default": 0, "min": 0, "max": 0xffffffffffffffff}),
                "unload_after": ("BOOLEAN", {"default": False}),
            },
            "optional": {
                # [ä¿®æ”¹ç‚¹2] image ç°åœ¨æ˜¯å¯é€‰çš„äº†ï¼
                "image": ("IMAGE",),
                "image_2": ("IMAGE",),
                "image_3": ("IMAGE",),
                "video_frames": ("IMAGE",), 
                "system_prompt": ("STRING", {"multiline": True, "default": "You are a helpful AI assistant."}),
                "base_url": ("STRING", {"default": "http://localhost:1234/v1"}),
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("response_text",)
    FUNCTION = "generate_content"
    CATEGORY = "PPP_nodes/LM Studio"

    def process_image(self, tensor_img, max_side):
        try:
            img_np = (tensor_img.cpu().numpy() * 255).clip(0, 255).astype(np.uint8)
            pil_img = Image.fromarray(img_np)
            if pil_img.mode != 'RGB': pil_img = pil_img.convert('RGB')
            width, height = pil_img.size
            if max(width, height) > max_side:
                ratio = max_side / max(width, height)
                new_size = (int(width * ratio), int(height * ratio))
                pil_img = pil_img.resize(new_size, Image.Resampling.LANCZOS)
            buffer = io.BytesIO()
            # æ”¹ä¸º PNG ä»¥è·å¾—æ›´å¥½çš„å…¼å®¹æ€§ï¼Œè™½ç„¶ä½“ç§¯ç¨å¤§
            pil_img.save(buffer, format="PNG")
            return base64.b64encode(buffer.getvalue()).decode('utf-8')
        except Exception as e:
            logger.error(f"Image processing error: {e}")
            return None

    def generate_content(self, user_prompt, model_name, max_total_images, gpu_offload, context_length, max_image_side,
                         max_tokens, temperature, seed, unload_after, 
                         image=None, image_2=None, image_3=None, video_frames=None,
                         system_prompt="", base_url="http://localhost:1234/v1", **kwargs):
        
        if "http" not in base_url: base_url = "http://localhost:1234/v1"
        IDENTIFIER = "comfy_vlm_worker"
        
        # 1. æ”¶é›†å›¾ç‰‡
        all_tensors = []
        if image is not None:
            for i in range(image.shape[0]): all_tensors.append(image[i])
        if image_2 is not None:
            for i in range(image_2.shape[0]): all_tensors.append(image_2[i])
        if image_3 is not None:
            for i in range(image_3.shape[0]): all_tensors.append(image_3[i])
        if video_frames is not None:
            for i in range(video_frames.shape[0]): all_tensors.append(video_frames[i])
        
        total_count = len(all_tensors)
        
        # --- [ä¿®æ”¹ç‚¹]ï¼šä¸å†å› ä¸º total_count == 0 è€ŒæŠ¥é”™ï¼Œè€Œæ˜¯è®°å½•æ—¥å¿— ---
        if total_count == 0:
            logger.info("No images detected. Running in Text-Only (Chat) mode.")
        else:
            logger.info(f"Processing {total_count} images for Vision mode.")
        
        # 2. æŠ½å¸§ (ä»…å½“æœ‰å›¾ç‰‡æ—¶æ‰§è¡Œ)
        final_tensors = []
        if total_count > 0:
            if total_count > max_total_images:
                indices = np.linspace(0, total_count - 1, max_total_images, dtype=int)
                final_tensors = [all_tensors[i] for i in indices]
            else:
                final_tensors = all_tensors

        # 3. è½¬ Base64 (ä»…å½“æœ‰å›¾ç‰‡æ—¶æ‰§è¡Œ)
        image_content_list = []
        if final_tensors:
            for tensor in final_tensors:
                b64 = self.process_image(tensor, max_image_side)
                if b64:
                    image_content_list.append({
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/png;base64,{b64}",
                            "detail": "auto" # æ˜¾å¼æ·»åŠ  detail å‚æ•°
                        }
                    })

        # 4. åŠ è½½æ¨¡å‹ (ä¿æŒä¸å˜)
        needs_reload = (
            LMS_VisionController._current_loaded_model != model_name or
            abs(LMS_VisionController._current_gpu_ratio - gpu_offload) > 0.01 or 
            LMS_VisionController._current_context != context_length
        )

        if needs_reload:
            self.cli.unload_all()
            time.sleep(1.0)
            success = self.cli.load_model(model_name, IDENTIFIER, gpu_ratio=gpu_offload, context_length=context_length)
            if success:
                LMS_VisionController._current_loaded_model = model_name
                LMS_VisionController._current_gpu_ratio = gpu_offload
                LMS_VisionController._current_context = context_length
                time.sleep(2.0)
            else:
                return (f"Error: Failed to load model '{model_name}'.",)

        # 5. æ„å»º Payload [æ ¸å¿ƒä¿®æ”¹ï¼šåŒºåˆ†çº¯æ–‡æœ¬å’Œå¤šæ¨¡æ€]
        user_content = ""
        
        if len(image_content_list) > 0:
            # è§†è§‰æ¨¡å¼ï¼šcontent æ˜¯ä¸€ä¸ªåˆ—è¡¨ [{"type":"text"}, {"type":"image_url"}...]
            user_content = [{"type": "text", "text": user_prompt}] + image_content_list
        else:
            # çº¯æ–‡æœ¬æ¨¡å¼ï¼šcontent åªæ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²
            # è¿™æ ·å…¼å®¹æ€§æœ€å¥½ï¼Œèƒ½æ”¯æŒä¸æ”¯æŒ Vision çš„çº¯æ–‡æœ¬æ¨¡å‹ (å¦‚ Llama 3, Mistral)
            user_content = user_prompt

        payload = {
            "model": IDENTIFIER,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content}
            ],
            "temperature": temperature,
            "max_tokens": max_tokens,
            "seed": seed,
            "stream": False
        }

        # 6. å‘é€è¯·æ±‚
        content = ""
        try:
            api_endpoint = f"{base_url.rstrip('/')}/chat/completions"
            
            # æ‰“å°æ—¥å¿—è®©ç”¨æˆ·çŸ¥é“ç°åœ¨æ˜¯ä»€ä¹ˆæ¨¡å¼
            mode_str = "Vision Mode" if len(image_content_list) > 0 else "Text-Only Mode"
            logger.info(f"Sending request ({mode_str})...")
            
            response = requests.post(api_endpoint, headers={"Content-Type": "application/json"}, json=payload, timeout=300)
            if response.status_code == 200:
                result = response.json()
                if 'choices' in result and len(result['choices']) > 0:
                    content = result['choices'][0]['message']['content']
                else:
                    content = "Error: Empty response."
            else:
                content = f"API Error {response.status_code}: {response.text}"
                logger.error(content)
        except Exception as e:
            content = f"Connection Error: {str(e)}"
            logger.error(content)

        if unload_after:
            self.cli.unload_all()
            LMS_VisionController._current_loaded_model = None

        return (content,)

# ==========================================
# æ–°å¢åŠŸèƒ½ï¼šPrompt ç®¡ç†ç³»ç»Ÿ
# ==========================================

# å®šä¹‰ prompt å­˜å‚¨çš„æ ¹ç›®å½• (åœ¨å½“å‰èŠ‚ç‚¹æ–‡ä»¶å¤¹ä¸‹è‡ªåŠ¨åˆ›å»º prompts æ–‡ä»¶å¤¹)
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PROMPTS_DIR = os.path.join(CURRENT_DIR, "prompts")

# å¦‚æœæ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œè‡ªåŠ¨åˆ›å»º
if not os.path.exists(PROMPTS_DIR):
    try:
        os.makedirs(PROMPTS_DIR)
        logger.info(f"Created prompts directory at: {PROMPTS_DIR}")
    except Exception as e:
        logger.error(f"Failed to create prompts directory: {e}")

class LMS_LoadPrompt:
    """
    è¯»å–èŠ‚ç‚¹ç›®å½•ä¸‹çš„ prompt æ–‡ä»¶ (.txt, .json)
    æ”¯æŒå­æ–‡ä»¶å¤¹ï¼Œæ”¯æŒä¸‹æ‹‰æœç´¢
    """
    @classmethod
    def INPUT_TYPES(cls):
        # æ¯æ¬¡åŠ è½½èŠ‚ç‚¹æ—¶ï¼Œéå†ç›®å½•è·å–æ–‡ä»¶åˆ—è¡¨
        files = []
        if os.path.exists(PROMPTS_DIR):
            for root, dirs, files_in_dir in os.walk(PROMPTS_DIR):
                for file in files_in_dir:
                    if file.lower().endswith((".txt", ".json")):
                        # è·å–ç›¸å¯¹è·¯å¾„ï¼Œä¾‹å¦‚ "é£æ ¼\èµ›åšæœ‹å…‹.txt"
                        full_path = os.path.join(root, file)
                        rel_path = os.path.relpath(full_path, PROMPTS_DIR)
                        files.append(rel_path)
        
        # æ’åºï¼Œä¿è¯åˆ—è¡¨æ•´é½
        files.sort()
        
        if not files:
            files = ["No prompts found.txt"]

        return {
            "required": {
                "prompt_file": (files,),
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("prompt_text",)
    FUNCTION = "load_file"
    CATEGORY = "PPP_nodes/Prompt"

    def load_file(self, prompt_file):
        file_path = os.path.join(PROMPTS_DIR, prompt_file)
        
        content = ""
        if not os.path.exists(file_path):
            logger.warning(f"File not found: {file_path}")
            return ("",)

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            logger.info(f"Loaded prompt from: {prompt_file}")
        except Exception as e:
            logger.error(f"Error reading file: {e}")
            content = f"Error reading file: {str(e)}"

        return (content,)

class LMS_SavePrompt:
    """
    ä¿å­˜æ–‡æœ¬åˆ°æ–‡ä»¶
    æ”¯æŒè‡ªåŠ¨åˆ›å»ºå­æ–‡ä»¶å¤¹ (ä¾‹å¦‚è¾“å…¥: è¯ç¥\åæ¨.txt)
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "text": ("STRING", {"forceInput": True}),  # æ¥æ”¶æ¥è‡ªå…¶ä»–èŠ‚ç‚¹çš„æ–‡æœ¬
                "filename": ("STRING", {"default": "folder/my_prompt.txt", "multiline": False}),
            },
            "optional": {
                "mode": (["overwrite", "append"],), # è¦†ç›–æ¨¡å¼ æˆ– è¿½åŠ æ¨¡å¼
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("saved_text",)
    OUTPUT_NODE = True
    FUNCTION = "save_file"
    CATEGORY = "PPP_nodes/Prompt"

    def save_file(self, text, filename, mode="overwrite"):
        # è§„èŒƒåŒ–è·¯å¾„ï¼Œå¤„ç† Windows çš„åæ–œæ 
        filename = filename.replace("\\", "/")
        
        # é˜²æ­¢ä¿å­˜åˆ°çˆ¶ç›®å½• (å®‰å…¨æ£€æŸ¥)
        if ".." in filename:
            logger.warning("Attempted path traversal. Saving to root instead.")
            filename = os.path.basename(filename)

        full_path = os.path.join(PROMPTS_DIR, filename)
        
        # ç¡®ä¿å­æ–‡ä»¶å¤¹å­˜åœ¨
        directory = os.path.dirname(full_path)
        if directory and not os.path.exists(directory):
            try:
                os.makedirs(directory)
                logger.info(f"Created sub-directory: {directory}")
            except Exception as e:
                logger.error(f"Failed to create directory: {e}")
                return (text,)

        # å†™å…¥æ–‡ä»¶
        try:
            write_mode = 'w' if mode == "overwrite" else 'a'
            # å¦‚æœæ˜¯è¿½åŠ æ¨¡å¼ï¼Œå…ˆåŠ ä¸ªæ¢è¡Œç¬¦
            content_to_write = text
            if mode == "append" and os.path.exists(full_path):
                content_to_write = "\n" + text

            with open(full_path, write_mode, encoding='utf-8') as f:
                f.write(content_to_write)
            
            logger.info(f"Saved prompt to: {full_path}")
        except Exception as e:
            logger.error(f"Error saving file: {e}")

        return (text,)

# ==========================================
# æ³¨å†ŒèŠ‚ç‚¹ (è¯·æ›´æ–°åŸæœ¬åº•éƒ¨çš„ MAPPINGS)
# ==========================================

# 1. æ‰¾åˆ°ä½ åŸæœ¬ä»£ç é‡Œçš„ NODE_CLASS_MAPPINGS = { ... }
# 2. å°†å…¶æ›¿æ¢æˆ–åˆå¹¶ä¸ºä»¥ä¸‹å†…å®¹ï¼š

NODE_CLASS_MAPPINGS = {
    "LMS_VisionController": LMS_VisionController,
    "LMS_LoadPrompt": LMS_LoadPrompt,
    "LMS_SavePrompt": LMS_SavePrompt
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "LMS_VisionController": "LM Studio VLM",
    "LMS_LoadPrompt": "ğŸ“‚ Load Prompt",
    "LMS_SavePrompt": "ğŸ’¾ Save Prompt"
}




